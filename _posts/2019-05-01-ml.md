---
layout:     post
title: 从零实现一个神经网络
subtitle: 分析与cpp/python实现
date:       2019-04-17
author:     Loopy
header-img: img/post-bg-2015.jpg
catalog: true
tags:
    - Algorithm
    - AI
    - Translate

---
<script type="text/javascript" async src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
> 本文内容大致翻译自[Machine Learning for Beginners: An Introduction to Neural Networks](https://victorzhou.com/blog/intro-to-neural-networks/)，我个人添加了C++的实现代码，以及部分内容，若感觉价值不高，请跳过或参阅原文。

> 本文代码可见[Github](TODO)。

# 面向初学者的机器学习教程：神经网络简介
**简单解释神经网络如何工作，以及从零开始使用Python和C++完成实现。**

可能会让你大吃一惊的是:神经网络并没有你想象的那么复杂!“神经网络”这个词经常被用作流行语，但实际上它比人们往往想象的要简单得多。

这篇文章是为完全初学者准备的，即使零基础的小白也是适用的。我们将了解神经网络如何工作，并从零开始使用Python和C++完成实现。

现在开始我们的教程！

## 1. 制作一个神经元

### 1.1 什么是神经元？
首先，我们要知道什么是神经元。它是神经网络的基本单位，用来接受输入，并做一些数学运算，然后产生一个输出。这是一个2输入神经元的示意图:
![neurons.png](./pic/2019-04-17/1.png)

你完全可以把它想象成一个函数，在这个神经元（函数）里发生了三个运算：
1. 每个输入分别乘以一个**权重w1,w2**（weight）
    $$x_1 → x_1 * w_1$$
    $$x_2 → x_2 * w_2$$
2. 将上一步的结果加起来，再加上一个**偏置b**（bias）
   $$(x_1 × w_1)+(x_2 × w_2)+ b$$
3. 把上一步的结果带入**激活函数**（activation function）中进行计算。于是就可以把输出表示为：
   $$y= f(x_1 ∗ w_1 + x_2 ∗ w_2 + b)$$

看到这里，你可能有两个疑问：
1. 激活函数f(x)是什么？
   
   激活函数的作用是将无限制的输入转换为可预测形式的输出。它是一类函数的总称，其中一种很常用的激活函数是sigmoid函数，它能将变量映射到0,1之间，的表达式为
    $$f(x) = \frac{1}{1+e^{-x}}$$
    在坐标轴上表现是这样的 ![sigmoid.png](./pic/2019-04-17/2.png)

    sigmoid函数的输出介于0和1，我们可以理解为它把 (−∞,+∞) 范围内的数压缩到 (0, 1)以内。正值越大输出越接近1，负向数值越大输出越接近0。

2. 神经元不就是个函数吗？
   
   是的，神经元就是个函数。将上面的几步整理起来，如果你选取Sigmoid作激活函数的话，一个2输入的神经元就是：
   $$f(x_1,x_2) = \frac{1}{1+e^{x_1 ∗ w_1 + x_2 ∗ w_2 + b}}$$

### 1.2 神经元实例
接下来是一个简单的例子，以辅助理解什么是神经元：

假设我们有一个Sigmoid作激活函数的2输入的神经元，以及下面的参数：
$$ w=[0,1]$$ 
$$ b=4$$
$(w=[0,1]$是$w_1=0,w_2=1$使用向量书写时的样子)$ 

那么我们就可以使用这个神经元来做计算了，比如，我们输入x = [2,3],那么计算过程就是(使用向量点乘表示)：
$$ result = \frac{1}{1+e^{x_1 ∗ w_1 + x_2 ∗ w_2 + b}}=\frac{1}{1+e^{[0,1]\cdot[2,3] +4}}\approx0.999$$
神经元在输入为[2,3]的时候输出为0.999。这样计算是从输入向前传递以获得输出，在神经网络中称为前馈过程。

### 1.3 神经元代码实现
是时候实现一个神经元了!我们这里使用了NumPy来完成一些数学计算:
```python
import numpy as np

def sigmoid(x):
  # Sigmoid激活函数：f(x) = 1 / (1 + e^(-x))
  return 1 / (1 + np.exp(-x))

class Neuron:
  def __init__(self, weights, bias):
    self.weights = weights
    self.bias = bias

  def feedforward(self, inputs):
    # 给输入赋权，添加偏置，然后带入激活函数
    total = np.dot(self.weights, inputs) + self.bias
    return sigmoid(total)
```
然后运行一下测试：
```python
weights = np.array([0, 1]) # w1 = 0, w2 = 1
bias = 4                   # b = 4
n = Neuron(weights, bias)

x = np.array([2, 3])       # x1 = 2, x2 = 3
print(n.feedforward(x))    # 0.9990889488055994
```

## 2. 连接神经元，搭建神经网络

### 2.1 什么是神经网络?

神经网络只不过是一群连接在一起的神经元。下面是一个简单的神经网络的示意图:
![network.png](./pic/2019-04-17/3.png)

这个网络有一个包含两个输入的输入层(Input Layer)，一个包括两个神经元(h1,h2)的隐藏层(Hidden Layer)，一个包含一个神经元(o1)的输出层(Output Layer)。连线表示o1的输入就是h1,h2的输出。

正是这种上层输出作为下层输入的特性，连接起了神经网络。

**隐藏层**(Hidden Layer)：输入层和输出层之间的所有层(Layer)都叫隐藏层，一个神经网络中可能有多个隐藏层。

### 2.2 神经网络上的前馈实例
**前馈**(feedforward):从输入向前传递以获得输出的过程(简单来说就是从左往右计算)

我们就使用上图所示的神经网络，并假设所有神经元的权重都为[0,1],偏置都为0，激活都使用Sigmoid函数。

那么在两个输入都为[2,3]的时候，计算输出的过程就为：

$$Result_{h_1}= Sigmoid(w\cdot x+b)=Sigmoid([0,1]\cdot [2,3]+0)=Sigmoid(3)=0.9526$$
$$Result_{h_2}= Result_{h_1}$$
$$Result_{o_1}= Sigmoid(w\cdot [Result_{h_1},Result_{h_2}]+b)=Sigmoid([0,1]\cdot [0.9526,0.9526]+0)=Sigmoid(0.9526)=0.7216$$

$$
\begin{equation}\begin{split} 
a&=b+c-d \\ 
&\quad +e-f\\ 
&=g+h\\ 
& =i 
\end{split}\end{equation}
$$